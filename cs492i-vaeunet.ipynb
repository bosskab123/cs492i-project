{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-12T18:16:30.743272Z","iopub.status.busy":"2021-12-12T18:16:30.742515Z","iopub.status.idle":"2021-12-12T18:16:34.024274Z","shell.execute_reply":"2021-12-12T18:16:34.023452Z","shell.execute_reply.started":"2021-12-12T18:16:30.743181Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import cv2\n","from tqdm import tqdm \n","import numpy as np\n","import os\n","import re\n","from easydict import EasyDict as edict\n","from PIL import Image\n","from skimage import io, transform\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch import Tensor\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.utils import save_image"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:34.463128Z","iopub.status.busy":"2021-12-12T18:16:34.462686Z","iopub.status.idle":"2021-12-12T18:16:34.513079Z","shell.execute_reply":"2021-12-12T18:16:34.512316Z","shell.execute_reply.started":"2021-12-12T18:16:34.463090Z"},"trusted":true},"outputs":[],"source":["mask_path = '../input/face-mask-lite-dataset/with_mask'\n","face_path = '../input/face-mask-lite-dataset/without_mask'\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:34.515257Z","iopub.status.busy":"2021-12-12T18:16:34.515064Z","iopub.status.idle":"2021-12-12T18:16:34.522204Z","shell.execute_reply":"2021-12-12T18:16:34.521547Z","shell.execute_reply.started":"2021-12-12T18:16:34.515232Z"},"trusted":true},"outputs":[],"source":["args = edict()\n","args.EPOCHS = 10\n","args.BATCH_SIZE=50\n","args.LR = 0.0002\n","args.B1 = 0.5\n","args.B2 = 0.999\n","args.N_CPU = 9\n","args.LATENT_DIM = 100\n","args.IMG_SIZE = 256\n","args.CHANNELS = 3\n","args.NUM_IMG = 10000\n","args.TRAINING_SIZE = int(0.9*args.NUM_IMG)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:34.533296Z","iopub.status.busy":"2021-12-12T18:16:34.532864Z","iopub.status.idle":"2021-12-12T18:16:34.552167Z","shell.execute_reply":"2021-12-12T18:16:34.551347Z","shell.execute_reply.started":"2021-12-12T18:16:34.533258Z"},"trusted":true},"outputs":[],"source":["class FaceTrainDataset(Dataset):\n","    def __init__(self, face_path, mask_path):\n","        def sorted_alphanumeric(data):  \n","            convert = lambda text: int(text) if text.isdigit() else text.lower()\n","            alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n","            return sorted(data,key = alphanum_key)\n","        \n","        self.transforms = transforms.Compose(\n","                            [transforms.Resize([args.IMG_SIZE, args.IMG_SIZE]),\n","                             transforms.ToTensor(),])\n","        \n","        self.face_path = face_path\n","        self.mask_path = mask_path\n","        self.face_file = sorted_alphanumeric(os.listdir(face_path))[:args.TRAINING_SIZE]\n","        self.mask_file = sorted_alphanumeric(os.listdir(mask_path))[:args.TRAINING_SIZE]\n","\n","    def __len__(self):\n","        return len(self.face_file)\n","\n","    def __getitem__(self, idx):\n","        face_image = Image.open(self.face_path + '/' + self.face_file[idx])\n","        mask_image = Image.open(self.mask_path + '/' + self.mask_file[idx])\n","        face_image = self.transforms(face_image)\n","        mask_image = self.transforms(mask_image)\n","\n","        return (face_image,mask_image)\n","\n","class FaceTestDataset(Dataset):\n","    def __init__(self, face_path, mask_path):\n","        def sorted_alphanumeric(data):  \n","            convert = lambda text: int(text) if text.isdigit() else text.lower()\n","            alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n","            return sorted(data,key = alphanum_key)\n","        \n","        self.transforms = transforms.Compose(\n","                            [transforms.Resize([args.IMG_SIZE, args.IMG_SIZE]),\n","                             transforms.ToTensor(),])\n","        \n","        self.face_path = face_path\n","        self.mask_path = mask_path\n","        self.face_file = sorted_alphanumeric(os.listdir(face_path))[args.TRAINING_SIZE:]\n","        self.mask_file = sorted_alphanumeric(os.listdir(mask_path))[args.TRAINING_SIZE:]\n","\n","    def __len__(self):\n","        return len(self.face_file)\n","\n","    def __getitem__(self, idx):\n","        face_image = Image.open(self.face_path + '/' + self.face_file[idx])\n","        mask_image = Image.open(self.mask_path + '/' + self.mask_file[idx])\n","        face_image = self.transforms(face_image)\n","        mask_image = self.transforms(mask_image)\n","\n","        return (face_image,mask_image)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:34.553969Z","iopub.status.busy":"2021-12-12T18:16:34.553646Z","iopub.status.idle":"2021-12-12T18:16:34.808999Z","shell.execute_reply":"2021-12-12T18:16:34.808212Z","shell.execute_reply.started":"2021-12-12T18:16:34.553932Z"},"trusted":true},"outputs":[],"source":["train_dataset = FaceTrainDataset(face_path=face_path, mask_path=mask_path)\n","train_dataloader = DataLoader(train_dataset, batch_size=args.BATCH_SIZE)\n","test_dataset = FaceTestDataset(face_path=face_path, mask_path=mask_path)\n","test_dataloader = DataLoader(test_dataset, batch_size=args.BATCH_SIZE)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:34.811226Z","iopub.status.busy":"2021-12-12T18:16:34.810717Z","iopub.status.idle":"2021-12-12T18:16:35.984207Z","shell.execute_reply":"2021-12-12T18:16:35.983441Z","shell.execute_reply.started":"2021-12-12T18:16:34.811185Z"},"trusted":true},"outputs":[],"source":["for i in range(len(train_dataset)):\n","    sample = train_dataset[i]\n","    ax = plt.subplot(1, 4, i + 1)\n","    plt.tight_layout()\n","    ax.set_title('Sample #{}'.format(i))\n","    ax.axis('off')\n","    if i % 2:\n","        sample_img = np.transpose(sample[0].cpu().detach().numpy(), (1,2,0))\n","    else:\n","        sample_img = np.transpose(sample[1].cpu().detach().numpy(), (1,2,0))\n","    plt.imshow(sample_img)\n","\n","    if i == 3:\n","        plt.show()\n","        break"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:35.986004Z","iopub.status.busy":"2021-12-12T18:16:35.985579Z","iopub.status.idle":"2021-12-12T18:16:35.995928Z","shell.execute_reply":"2021-12-12T18:16:35.995203Z","shell.execute_reply.started":"2021-12-12T18:16:35.985965Z"},"trusted":true},"outputs":[],"source":["class DownSampleConv(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True):\n","        \"\"\"\n","        Paper details:\n","        - C64-C128-C256-C512-C512-C512-C512-C512\n","        - All convolutions are 4×4 spatial filters applied with stride 2\n","        - Convolutions in the encoder downsample by a factor of 2\n","        \"\"\"\n","        super().__init__()\n","        self.activation = activation\n","        self.batchnorm = batchnorm\n","\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding)\n","\n","        if batchnorm:\n","            self.bn = nn.BatchNorm2d(out_channels)\n","\n","        if activation:\n","            self.act = nn.LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.batchnorm:\n","            x = self.bn(x)\n","        if self.activation:\n","            x = self.act(x)\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:35.999089Z","iopub.status.busy":"2021-12-12T18:16:35.998812Z","iopub.status.idle":"2021-12-12T18:16:36.011450Z","shell.execute_reply":"2021-12-12T18:16:36.010537Z","shell.execute_reply.started":"2021-12-12T18:16:35.999051Z"},"trusted":true},"outputs":[],"source":["class UpSampleConv(nn.Module):\n","\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel=4,\n","        strides=2,\n","        padding=1,\n","        activation=True,\n","        batchnorm=True,\n","        dropout=False\n","    ):\n","        super().__init__()\n","        self.activation = activation\n","        self.batchnorm = batchnorm\n","        self.dropout = dropout\n","\n","        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n","\n","        if batchnorm:\n","            self.bn = nn.BatchNorm2d(out_channels)\n","\n","        if activation:\n","            self.act = nn.ReLU(True)\n","\n","        if dropout:\n","            self.drop = nn.Dropout2d(0.5)\n","\n","    def forward(self, x):\n","        x = self.deconv(x)\n","        if self.batchnorm:\n","            x = self.bn(x)\n","\n","        if self.dropout:\n","            x = self.drop(x)\n","        return x"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:36.013502Z","iopub.status.busy":"2021-12-12T18:16:36.013229Z","iopub.status.idle":"2021-12-12T18:16:36.027914Z","shell.execute_reply":"2021-12-12T18:16:36.027071Z","shell.execute_reply.started":"2021-12-12T18:16:36.013468Z"},"trusted":true},"outputs":[],"source":["class Generator(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels):\n","        \"\"\"\n","        Paper details:\n","        - Encoder: C64-C128-C256-C512-C512-C512-C512-C512\n","        - All convolutions are 4×4 spatial filters applied with stride 2\n","        - Convolutions in the encoder downsample by a factor of 2\n","        - Decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128\n","        \"\"\"\n","        super().__init__()\n","\n","        # encoder/donwsample convs\n","        self.encoders = [\n","            DownSampleConv(in_channels, 64, batchnorm=False),  # bs x 64 x 128 x 128\n","            DownSampleConv(64, 128),  # bs x 128 x 64 x 64\n","            DownSampleConv(128, 256),  # bs x 256 x 32 x 32\n","            DownSampleConv(256, 512),  # bs x 512 x 16 x 16\n","            DownSampleConv(512, 512),  # bs x 512 x 8 x 8\n","            DownSampleConv(512, 512),  # bs x 512 x 4 x 4\n","            DownSampleConv(512, 512),  # bs x 512 x 2 x 2\n","            DownSampleConv(512, 512, batchnorm=False),  # bs x 512 x 1 x 1\n","        ]\n","\n","        # decoder/upsample convs\n","        self.decoders = [\n","            UpSampleConv(512, 512, dropout=True),  # bs x 512 x 2 x 2\n","            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 4 x 4\n","            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 8 x 8\n","            UpSampleConv(1024, 512),  # bs x 512 x 16 x 16\n","            UpSampleConv(1024, 256),  # bs x 256 x 32 x 32\n","            UpSampleConv(512, 128),  # bs x 128 x 64 x 64\n","            UpSampleConv(256, 64),  # bs x 64 x 128 x 128\n","        ]\n","        self.decoder_channels = [512, 512, 512, 512, 256, 128, 64]\n","        self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1)\n","        self.tanh = nn.Tanh()\n","\n","        self.encoders = nn.ModuleList(self.encoders)\n","        self.decoders = nn.ModuleList(self.decoders)\n","\n","    def forward(self, x):\n","        skips_cons = []\n","        for encoder in self.encoders:\n","            x = encoder(x)\n","\n","            skips_cons.append(x)\n","\n","        skips_cons = list(reversed(skips_cons[:-1]))\n","        decoders = self.decoders[:-1]\n","\n","        for decoder, skip in zip(decoders, skips_cons):\n","            x = decoder(x)\n","            x = torch.cat((x, skip), axis=1)\n","\n","        x = self.decoders[-1](x)\n","        x = self.final_conv(x)\n","        return self.tanh(x)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:36.029484Z","iopub.status.busy":"2021-12-12T18:16:36.029205Z","iopub.status.idle":"2021-12-12T18:16:39.548377Z","shell.execute_reply":"2021-12-12T18:16:39.547619Z","shell.execute_reply.started":"2021-12-12T18:16:36.029444Z"},"trusted":true},"outputs":[],"source":["l1_loss = nn.L1Loss()\n","mse_loss = nn.MSELoss()\n","\n","generator = Generator(3,3).to(device)\n","optimizer_G = torch.optim.Adam(generator.parameters(), lr=args.LR, betas=(args.B1, args.B2))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def _weights_init(m):\n","    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:39.555069Z","iopub.status.busy":"2021-12-12T18:16:39.553059Z","iopub.status.idle":"2021-12-12T18:16:39.575958Z","shell.execute_reply":"2021-12-12T18:16:39.575319Z","shell.execute_reply.started":"2021-12-12T18:16:39.555018Z"},"trusted":true},"outputs":[],"source":["generator.apply(_weights_init)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:39.581910Z","iopub.status.busy":"2021-12-12T18:16:39.579932Z","iopub.status.idle":"2021-12-12T18:16:39.587392Z","shell.execute_reply":"2021-12-12T18:16:39.586787Z","shell.execute_reply.started":"2021-12-12T18:16:39.581869Z"},"trusted":true},"outputs":[],"source":["os.makedirs('./saved_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T18:16:39.594334Z","iopub.status.busy":"2021-12-12T18:16:39.591827Z"},"trusted":true},"outputs":[],"source":["train_loss = []\n","val_loss = []\n","for epoch in range(args.EPOCHS):\n","    train_loss_ = 0\n","    val_loss_ = 0\n","    generator.train()\n","    for i, (face_imgs, mask_imgs) in enumerate(train_dataloader):\n","        face_imgs = Variable(face_imgs.type(Tensor)).to(device)\n","        mask_imgs = Variable(mask_imgs.type(Tensor)).to(device)\n","\n","        optimizer_G.zero_grad()\n","        gen_imgs = generator(mask_imgs)\n","        loss = l1_loss(gen_imgs, face_imgs)\n","        train_loss_ += loss.item()\n","        loss.backward()\n","        optimizer_G.step()\n","\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [loss: %f]\"\n","            % (epoch, args.EPOCHS, i, len(train_dataloader), loss.item())\n","        )\n","        \n","        if i%20 == 0:\n","            ax = plt.subplot(2, 2, 1)\n","            plt.tight_layout()\n","            ax.set_title('Sample #{}'.format(i))\n","            ax.axis('off')\n","            sample_img = np.transpose(gen_imgs[0].cpu().detach().numpy(), (1,2,0))\n","            plt.imshow(sample_img)\n","\n","            ax = plt.subplot(2, 2, 2)\n","            plt.tight_layout()\n","            ax.set_title('Sample #{}'.format(i))\n","            ax.axis('off')\n","            sample_img = np.transpose(face_imgs[0].cpu().detach().numpy(), (1,2,0))\n","            plt.imshow(sample_img)\n","\n","            plt.show()\n","        \n","    train_loss_ /= len(train_dataloader)\n","    \n","    torch.save(generator.state_dict(), \"./saved_model/unet_{}.pth\".format(epoch))\n","    \n","    generator.eval()\n","    for i, (face_imgs, mask_imgs) in enumerate(test_dataloader):\n","        face_imgs = Variable(face_imgs.type(Tensor)).to(device)\n","        mask_imgs = Variable(mask_imgs.type(Tensor)).to(device)\n","        \n","        gen_imgs = generator(face_imgs)\n","        loss = l1_loss(gen_imgs, mask_imgs)\n","        val_loss_ += loss.item()\n","        print(\n","            \"[Epoch %d/%d] [Batch %d/%d] [loss: %f]\"\n","            % (epoch, args.EPOCHS, i, len(test_dataloader), loss.item())\n","        )\n","        \n","    val_loss_ /= len(test_dataloader)\n","    \n","    train_loss.append(train_loss_)\n","    val_loss.append(val_loss_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generator.load_state_dict(torch.load('../input/vae-unet/unet.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generator.eval()\n","sample = test_dataset[0]\n","ax = plt.subplot(1, 2, 1)\n","plt.tight_layout()\n","ax.set_title('Sample #{}'.format(i))\n","ax.axis('off')\n","mask_img = Variable(sample[1].type(Tensor)).to(device)\n","gen_img = generator(mask_img.unsqueeze(0))\n","sample_img = np.transpose(gen_img.cpu().detach().numpy().squeeze(0), (1,2,0))\n","plt.imshow(sample_img)\n","\n","ax = plt.subplot(1, 2, 2)\n","plt.tight_layout()\n","ax.set_title('Sample #{}'.format(i))\n","ax.axis('off')\n","sample_img = np.transpose(sample[0].cpu().detach().numpy(), (1,2,0))\n","plt.imshow(sample_img)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generator.eval()\n","sample = test_dataset[1]\n","ax = plt.subplot(1, 2, 1)\n","plt.tight_layout()\n","ax.set_title('Sample #{}'.format(i))\n","ax.axis('off')\n","mask_img = Variable(sample[1].type(Tensor)).to(device)\n","gen_img = generator(mask_img.unsqueeze(0))\n","sample_img = np.transpose(gen_img.cpu().detach().numpy().squeeze(0), (1,2,0))\n","plt.imshow(sample_img)\n","\n","ax = plt.subplot(1, 2, 2)\n","plt.tight_layout()\n","ax.set_title('Sample #{}'.format(i))\n","ax.axis('off')\n","sample_img = np.transpose(sample[0].cpu().detach().numpy(), (1,2,0))\n","plt.imshow(sample_img)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,20))\n","plt.plot(train_loss)\n","plt.plot(val_loss)\n","plt.legend([\"loss\",\"val_loss\"])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generator.eval()\n","total_mse_loss = 0\n","total_l1_loss = 0\n","for i, (face_imgs, mask_imgs) in enumerate(test_dataloader):\n","    Y_imgs = Variable(face_imgs.type(Tensor)).to(device)\n","    X_imgs = Variable(mask_imgs.type(Tensor)).to(device)\n","    gen_imgs = generator(X_imgs)\n","    total_mse_loss += mse_loss(gen_imgs, Y_imgs).item()\n","    total_l1_loss += l1_loss(gen_imgs, Y_imgs).item()\n","    \n","print(\"MSE loss: {}\".format(total_mse_loss/len(test_dataloader)))\n","print(\"L1 loss: {}\".format(total_l1_loss/len(test_dataloader)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
